Sender: LSF System <lsfadmin@polus-c1-ib.bmc.hpc.cs.msu.ru>
Subject: Job 303370: <mpiexec -n 32 ./src/mpiTest.py> in cluster <MSUCluster> Exited

Job <mpiexec -n 32 ./src/mpiTest.py> was submitted from host <polus-ib.bmc.hpc.cs.msu.ru> by user <kozlov> in cluster <MSUCluster> at Sat May 25 20:53:52 2019
Job was executed on host(s) <16*polus-c1-ib.bmc.hpc.cs.msu.ru>, in queue <short>, as user <kozlov> in cluster <MSUCluster> at Sat May 25 20:53:52 2019
</home/kozlov/> was used as the home directory.
</home/kozlov/prog/magadisser> was used as the working directory.
Started at Sat May 25 20:53:52 2019
Terminated at Sat May 25 20:53:52 2019
Results reported at Sat May 25 20:53:52 2019

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
mpiexec -n 32 ./src/mpiTest.py
------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   0.09 sec.
    Max Memory :                                 1 MB
    Average Memory :                             1.00 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              1
    Max Threads :                                1
    Run time :                                   2 sec.
    Turnaround time :                            0 sec.

The output (if any) follows:

--------------------------------------------------------------------------
There are not enough slots available in the system to satisfy the 32 slots
that were requested by the application:
  ./src/mpiTest.py

Either request fewer slots for your application, or make more slots available
for use.
--------------------------------------------------------------------------
